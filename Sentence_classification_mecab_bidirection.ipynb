{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification by MorphConv\n",
    "Implementation of [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) to classify sentiment of movie review\n",
    "\n",
    "### Explanation of this notebook\n",
    "* Dataset : [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc)\n",
    "    + train, validation : splitting `ratings_train.txt` (150k reviews) for train (120k reviews) and validation (30k reviews)\n",
    "    + test : `ratings_test.txt` (50k reviews)\n",
    "* Preprocessing\n",
    "    + Morphological analysis by Mecab wrapped by [konlpy](http://konlpy.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import (Input, Embedding, Conv1D, AveragePooling1D, \n",
    "                          concatenate, Bidirectional, GRU, Dense)\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "ratings_train = pd.read_csv('data/ratings_train.txt', sep = '\\t')[['document', 'label']]\n",
    "ratings_test = pd.read_csv('data/ratings_test.txt', sep = '\\t')[['document', 'label']]\n",
    "\n",
    "# ratings, ratings_tst의 document column에 nan 값이 있으므로 이를 빈 문자열로 대체\n",
    "print(sum(ratings_train.document.isna()), sum(ratings_test.document.isna()))\n",
    "\n",
    "ratings_train.document[ratings_train.document.isna()] = ''\n",
    "ratings_test.document[ratings_test.document.isna()] = ''\n",
    "\n",
    "print(sum(ratings_train.document.isna()), sum(ratings_test.document.isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use mecab for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_morphs(text):\n",
    "    text = re.sub('[^ㅎㅎ|^^|ㅡ|\\-|~|;|♥|♡|★|ㅠ|ㅜ|a-z|A-Z|ㄱ-ㅎ|가-힣|\\'|\\\"|\\,|\\.|\\!|\\?|\\d]', \n",
    "                  ' ', text)\n",
    "    text = re.sub('(ㅡ.ㅡ|-.-)', ' ㅡㅡ ', text)\n",
    "    text = re.sub('(ㅡ|-){2,}', ' ㅡㅡ ', text)\n",
    "    text = re.sub('(ㄱ-ㅎ|^){2,}', ' \\g<1>\\g<1> ', text)\n",
    "    text = re.sub('(♥|♡)+', ' ♥♥ ', text)\n",
    "    text = re.sub('(★|;|~)+', ' \\g<1>\\g<1> ', text)\n",
    "    text = re.sub('[ㅠ|ㅜ]+', ' ㅠ ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    result_word = mecab.morphs(text)\n",
    "#     result_word = [word for word in result_word if len(word) > 1]\n",
    "    result_words = '+'.join(result_word)\n",
    "#     result_words = result_words.replace('^+^', '^^')\n",
    "#     result_words = result_words.replace('ㅠ', 'ㅠㅠ')\n",
    "    return result_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train morphs......\n",
      "Make test morphs......\n",
      "CPU times: user 31 s, sys: 151 ms, total: 31.2 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "print('Make train morphs......')\n",
    "ratings_train['morphs'] = ratings_train['document'].apply(make_morphs)\n",
    "\n",
    "# test\n",
    "print('Make test morphs......')\n",
    "ratings_test['morphs'] = ratings_test['document'].apply(make_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.to_csv('data/ratings_train_mecab.txt', sep='\\t', index=False)\n",
    "ratings_test.to_csv('data/ratings_test_mecab.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 16\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('data/ratings_train_mecab.txt', sep = '\\t')[['morphs', 'label']]\n",
    "ratings_test = pd.read_csv('data/ratings_test_mecab.txt', sep = '\\t')[['morphs', 'label']]\n",
    "\n",
    "# ratings, ratings_tst의 document column에 nan 값이 있으므로 이를 빈 문자열로 대체\n",
    "print(sum(ratings.morphs.isna()), sum(ratings_test.morphs.isna()))\n",
    "\n",
    "ratings.morphs[ratings.morphs.isna()] = ''\n",
    "ratings_test.morphs[ratings_test.morphs.isna()] = ''\n",
    "\n",
    "print(sum(ratings.morphs.isna()), sum(ratings_test.morphs.isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best random seed trought rasidual and varience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find best seed......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c58f3664a0b4b9da4c8a190b2a77c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0\n",
      "280028\n",
      "73093.95535086525 72788.64036922259\n",
      "seed: 4\n",
      "279283\n",
      "72659.20509965229 74530.59328873192\n",
      "seed: 8\n",
      "277420\n",
      "72949.85140907383 73359.04609988058\n",
      "seed: 108\n",
      "276936\n",
      "73050.30235850641 72953.05609298393\n",
      "seed: 128\n",
      "276021\n",
      "73141.69375567777 72588.56850242332\n",
      "seed: 267\n",
      "275940\n",
      "73197.65236750114 72370.5584639531\n",
      "seed: 1069\n",
      "275647\n",
      "73067.3642571775 72884.91745579352\n",
      "seed: 1395\n",
      "275052\n",
      "73194.207448039 72384.66283194325\n",
      "seed: 4843\n",
      "274889\n",
      "73004.69947935347 73136.23770602426\n",
      "seed: 5107\n",
      "274502\n",
      "73130.34417974179 72631.76975883021\n"
     ]
    }
   ],
   "source": [
    "# print('Find best seed......')\n",
    "\n",
    "# min_seed = 0\n",
    "# min_residual = 100000000\n",
    "# for i in tqdm(range(10000)):\n",
    "#     x_data = ratings.morphs.apply(split_word).tolist()\n",
    "#     y_data = ratings.label.tolist()\n",
    "\n",
    "#     x_train_word, x_val_word, y_train, y_val = train_test_split(x_data, y_data,\n",
    "#                                                                 test_size=0.2,\n",
    "#                                                                 random_state=i,\n",
    "#                                                                 stratify=y_data)\n",
    "#     # print(len(y_train), sum(y_train), len(y_val), sum(y_val))\n",
    "\n",
    "#     word_table = list(set([word for words in x_data for word in words]))\n",
    "#     word_table = {word:0 for word in word_table}\n",
    "\n",
    "#     train_counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c in x_train_word]))\n",
    "#     train_table = word_table.copy()\n",
    "#     train_table.update(train_counter)\n",
    "\n",
    "#     val_counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c in x_val_word]))\n",
    "#     val_table = word_table.copy()\n",
    "#     val_table.update(val_counter)\n",
    "\n",
    "#     train_cnt = np.array(list(train_table.values()))\n",
    "#     val_cnt = np.array(list(val_table.values())) * 4\n",
    "    \n",
    "#     residual = np.abs(train_cnt-val_cnt).sum()\n",
    "#     if residual < min_residual:\n",
    "#         min_residual = residual\n",
    "#         min_seed = i\n",
    "#         print('seed:', i)\n",
    "#         print(min_residual)\n",
    "#         print(np.var(train_cnt), np.var(val_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(text):\n",
    "#     return text.split('+')\n",
    "    return [word for word in text.split('+')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train, val data......\n"
     ]
    }
   ],
   "source": [
    "print('Make train, val data......')\n",
    "\n",
    "x_data = ratings.morphs.apply(split_word).tolist()\n",
    "y_data = ratings.label.tolist()\n",
    "\n",
    "x_train_word, x_val_word, y_train, y_val = train_test_split(x_data, y_data,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=108,\n",
    "                                                            stratify=y_data)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.0, 150000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([len(k) for k in keywords]), len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_cnt = Counter([i for item in keywords for i in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_clip_dict = dict(keyword_clip)\n",
    "keyword_clip_dict = {key:value for key, value in keyword_cnt.items() if value >= 3}\n",
    "keyword_dict = dict(zip(keyword_clip_dict.keys(), range(len(keyword_clip_dict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#공백과 미학습 단어 처리를 위한 사전 정보 추가\n",
    "keyword_dict['_PAD_'] = len(keyword_dict)\n",
    "keyword_dict['_UNK_'] = len(keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰 시퀀스 단어수의 중앙값 +10를 max 리뷰 시퀀스로 정함... \n",
    "# max_seq = np.median([len(k) for k in keywords]) + 10\n",
    "max_seq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_and_padding(corp_list, dic, max_seq=50):\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    coding_seq = [ [dic.get(j, dic['_UNK_']) for j in i]  for i in corp_list ]\n",
    "    #일반적으로 리뷰는 마지막 부분에 많은 정보를 포함할 가능성이 많아 패딩은 앞에 준다. \n",
    "    return(pad_sequences(coding_seq, maxlen=max_seq, padding='pre', truncating='pre',value=dic['_PAD_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encoding_and_padding(x_train_word, keyword_dict, max_seq=int(max_seq))\n",
    "x_val = encoding_and_padding(x_val_word, keyword_dict, max_seq=int(max_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120000, 20), (120000,), (30000, 20), (30000,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MorphConv class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 50)       1117700     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 32)       1632        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 16)       1616        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 8)        1208        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_10 (AveragePo (None, 10, 32)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_11 (AveragePo (None, 10, 16)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_12 (AveragePo (None, 10, 8)        0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 10, 56)       0           average_pooling1d_10[0][0]       \n",
      "                                                                 average_pooling1d_11[0][0]       \n",
      "                                                                 average_pooling1d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 20)           4020        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            21          bidirectional_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,126,197\n",
      "Trainable params: 1,126,197\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(x_train.shape[1],), name='input')\n",
    "embeddings_out = Embedding(input_dim=len(keyword_dict), \n",
    "                           output_dim=50,\n",
    "                           name='embedding')(inputs)\n",
    "\n",
    "conv0 = Conv1D(32, 1, padding='same')(embeddings_out)\n",
    "conv1 = Conv1D(16, 2, padding='same')(embeddings_out)\n",
    "conv2 = Conv1D(8, 3, padding='same')(embeddings_out)\n",
    "\n",
    "pool0 = AveragePooling1D()(conv0)\n",
    "pool1 = AveragePooling1D()(conv1)\n",
    "pool2 = AveragePooling1D()(conv2)\n",
    "\n",
    "concat_layer = concatenate([pool0, pool1, pool2],axis=2)\n",
    "\n",
    "bidir = Bidirectional(GRU(10, recurrent_dropout=0.2, dropout=0.2))(concat_layer)\n",
    "\n",
    "out = Dense(1,activation='sigmoid')(bidir)\n",
    "\n",
    "model = Model(inputs=[inputs,], outputs=out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "#               optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "# lr = 0.003\n",
    "epochs = 30 # 30\n",
    "# batch_size = 10000\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoint/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/30\n",
      "120000/120000 [==============================] - 20s 169us/step - loss: 0.4742 - acc: 0.7862 - val_loss: 0.3683 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36826, saving model to checkpoint/01-0.3683.hdf5\n",
      "Epoch 2/30\n",
      "120000/120000 [==============================] - 15s 129us/step - loss: 0.3489 - acc: 0.8521 - val_loss: 0.3538 - val_acc: 0.8456\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36826 to 0.35385, saving model to checkpoint/02-0.3538.hdf5\n",
      "Epoch 3/30\n",
      "120000/120000 [==============================] - 15s 129us/step - loss: 0.3267 - acc: 0.8630 - val_loss: 0.3523 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35385 to 0.35227, saving model to checkpoint/03-0.3523.hdf5\n",
      "Epoch 4/30\n",
      "120000/120000 [==============================] - 16s 131us/step - loss: 0.3146 - acc: 0.8684 - val_loss: 0.3600 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35227\n",
      "Epoch 5/30\n",
      "120000/120000 [==============================] - 16s 132us/step - loss: 0.3054 - acc: 0.8733 - val_loss: 0.3548 - val_acc: 0.8456\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35227\n",
      "Epoch 6/30\n",
      "120000/120000 [==============================] - 16s 131us/step - loss: 0.2959 - acc: 0.8776 - val_loss: 0.3546 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35227\n",
      "Epoch 7/30\n",
      "120000/120000 [==============================] - 16s 131us/step - loss: 0.2880 - acc: 0.8816 - val_loss: 0.3557 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35227\n",
      "Epoch 8/30\n",
      "120000/120000 [==============================] - 16s 132us/step - loss: 0.2781 - acc: 0.8861 - val_loss: 0.3527 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35227\n",
      "Epoch 9/30\n",
      "120000/120000 [==============================] - 17s 140us/step - loss: 0.2690 - acc: 0.8902 - val_loss: 0.3571 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35227\n",
      "Epoch 10/30\n",
      "120000/120000 [==============================] - 16s 137us/step - loss: 0.2598 - acc: 0.8948 - val_loss: 0.3572 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35227\n",
      "Epoch 11/30\n",
      "120000/120000 [==============================] - 16s 134us/step - loss: 0.2516 - acc: 0.8988 - val_loss: 0.3626 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35227\n",
      "Epoch 12/30\n",
      "120000/120000 [==============================] - 16s 131us/step - loss: 0.2436 - acc: 0.9023 - val_loss: 0.3663 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35227\n",
      "Epoch 13/30\n",
      "120000/120000 [==============================] - 16s 131us/step - loss: 0.2361 - acc: 0.9059 - val_loss: 0.3698 - val_acc: 0.8474\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35227\n",
      "Epoch 14/30\n",
      "120000/120000 [==============================] - 16s 132us/step - loss: 0.2292 - acc: 0.9098 - val_loss: 0.3751 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35227\n",
      "Epoch 15/30\n",
      "120000/120000 [==============================] - 17s 143us/step - loss: 0.2225 - acc: 0.9118 - val_loss: 0.3808 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35227\n",
      "Epoch 16/30\n",
      "120000/120000 [==============================] - 16s 133us/step - loss: 0.2157 - acc: 0.9155 - val_loss: 0.3907 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.35227\n",
      "Epoch 17/30\n",
      "120000/120000 [==============================] - 16s 131us/step - loss: 0.2096 - acc: 0.9179 - val_loss: 0.3909 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.35227\n",
      "Epoch 18/30\n",
      "120000/120000 [==============================] - 16s 132us/step - loss: 0.2032 - acc: 0.9213 - val_loss: 0.3949 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.35227\n",
      "Epoch 19/30\n",
      "120000/120000 [==============================] - 16s 132us/step - loss: 0.1977 - acc: 0.9233 - val_loss: 0.4061 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.35227\n",
      "Epoch 20/30\n",
      "120000/120000 [==============================] - 16s 136us/step - loss: 0.1908 - acc: 0.9265 - val_loss: 0.4162 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.35227\n",
      "Epoch 21/30\n",
      " 32000/120000 [=======>......................] - ETA: 11s - loss: 0.1754 - acc: 0.9337"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-e577ec08657c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                   shuffle=True, callbacks = [checkpointer] )\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#                  shuffle=True, callbacks = [checkpointer, early_stopping] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/5th_modulab_sentiment_analysis/vir_nlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Project/5th_modulab_sentiment_analysis/vir_nlp/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/5th_modulab_sentiment_analysis/vir_nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/5th_modulab_sentiment_analysis/vir_nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/5th_modulab_sentiment_analysis/vir_nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit( x=x_train, y=y_train, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=[x_val, y_val],\n",
    "                  shuffle=True, callbacks = [checkpointer] )\n",
    "#                  shuffle=True, callbacks = [checkpointer, early_stopping] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make test data......\n"
     ]
    }
   ],
   "source": [
    "# Make test data\n",
    "print('Make test data......')\n",
    "x_test_word = ratings_test.morphs.apply(split_word).tolist()\n",
    "x_test = encoding_and_padding(x_test_word, keyword_dict, max_seq=int(max_seq))\n",
    "y_test = np.asarray(ratings_test.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 20), (50000,))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoint/'\n",
    "model = load_model(model_path+'03-0.3523.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 8s 156us/step\n",
      "Loss: 0.3612064360141754 Accuracy: 0.8433\n"
     ]
    }
   ],
   "source": [
    "# prob = model.predict(x_test)\n",
    "\n",
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8434158722545316\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "test_f1_score = f1_score(y_test, pred > 0.5)\n",
    "print('F1 Score:', test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
