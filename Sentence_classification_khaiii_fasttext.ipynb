{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification by MorphConv\n",
    "Implementation of [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) to classify sentiment of movie review\n",
    "\n",
    "### Explanation of this notebook\n",
    "* Dataset : [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc)\n",
    "    + train, validation : splitting `ratings_train.txt` (150k reviews) for train (120k reviews) and validation (30k reviews)\n",
    "    + test : `ratings_test.txt` (50k reviews)\n",
    "* Preprocessing\n",
    "    + Morphological analysis by Mecab wrapped by [konlpy](http://konlpy.org/en/latest/)\n",
    "    + Using [FastText](https://arxiv.org/abs/1607.04606) embedding by [gluonnlp package](https://gluon-nlp.mxnet.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "np.random.seed(7)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# import khaiii\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "ratings_train = pd.read_csv('data/ratings_train.txt', sep = '\\t')[['document', 'label']]\n",
    "ratings_test = pd.read_csv('data/ratings_test.txt', sep = '\\t')[['document', 'label']]\n",
    "\n",
    "# ratings, ratings_tst의 document column에 nan 값이 있으므로 이를 빈 문자열로 대체\n",
    "print(sum(ratings_train.document.isna()), sum(ratings_test.document.isna()))\n",
    "\n",
    "ratings_train.document[ratings_train.document.isna()] = ''\n",
    "ratings_test.document[ratings_test.document.isna()] = ''\n",
    "\n",
    "print(sum(ratings_train.document.isna()), sum(ratings_test.document.isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use khaiii for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_morphs(text):\n",
    "#     tag_list = ['NNG', 'NNP', 'NNB', 'NP', 'NR', 'VV', 'VA', \n",
    "#             'VX', 'VCP', 'VCN', 'MM', 'MAG', 'MAJ', 'IC']\n",
    "    tag_list = ['NNG', 'NNP', 'NP', 'NR', 'VV', 'VA', 'VX', 'VCP', 'VCN',\n",
    "                'MM', 'MAG', 'MAJ', 'IC', 'SN', 'SW', 'SWK', 'SO', 'XR',\n",
    "                'SH', 'SL', 'ZN', 'ZV', 'SP', 'SE']\n",
    "\n",
    "#     text = re.sub('[^ㅎㅎ|^^|ㅡ|\\-|~|;|♥|♡|★|ㅠ|ㅜ|a-z|A-Z|ㄱ-ㅎ|가-힣|\\'|\\\"|\\,|\\.|\\!|\\?|\\d]', \n",
    "#                   ' ', text)\n",
    "    text = re.sub('(ㅡ.ㅡ|-.-)', ' ㅡㅡ ', text)\n",
    "    text = re.sub('(ㅡ|-){2,}', ' ㅡㅡ ', text)\n",
    "    text = re.sub('(ㄱ-ㅎ|^){2,}', ' \\g<1>\\g<1> ', text)\n",
    "    text = re.sub('(♥|♡)+', ' ♥♥ ', text)\n",
    "    text = re.sub('(★|;|~)+', ' \\g<1>\\g<1> ', text)\n",
    "    text = re.sub('[ㅠ|ㅜ]+', ' ㅠ ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    result = api.analyze(text) \n",
    "    result_word = [m.lex+'다' if m.tag.startswith('V') else m.lex\n",
    "                       for word in result\n",
    "                          for m in word.morphs\n",
    "                            if m.tag in tag_list]\n",
    "    result_words = '+'.join(result_word)\n",
    "    result_words = result_words.replace('^+^', '^^')\n",
    "    result_words = result_words.replace('ㅠ', 'ㅠㅠ')\n",
    "    return result_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /usr/local/lib/python3.6/dist-packages/khaiii\n"
     ]
    }
   ],
   "source": [
    "api = khaiii.KhaiiiApi()\n",
    "api.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train morphs......\n",
      "Make test morphs......\n",
      "CPU times: user 3min 25s, sys: 400 ms, total: 3min 26s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "print('Make train morphs......')\n",
    "ratings_train['morphs'] = ratings_train['document'].apply(make_morphs)\n",
    "\n",
    "# test\n",
    "print('Make test morphs......')\n",
    "ratings_test['morphs'] = ratings_test['document'].apply(make_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.to_csv('data/ratings_train_khaiii.txt', sep='\\t', index=False)\n",
    "ratings_test.to_csv('data/ratings_test_khaiii.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 29\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('data/ratings_train_khaiii.txt', sep = '\\t')[['morphs', 'label']]\n",
    "ratings_test = pd.read_csv('data/ratings_test_khaiii.txt', sep = '\\t')[['morphs', 'label']]\n",
    "\n",
    "# ratings, ratings_tst의 document column에 nan 값이 있으므로 이를 빈 문자열로 대체\n",
    "print(sum(ratings.morphs.isna()), sum(ratings_test.morphs.isna()))\n",
    "\n",
    "ratings.morphs[ratings.morphs.isna()] = ''\n",
    "ratings_test.morphs[ratings_test.morphs.isna()] = ''\n",
    "\n",
    "print(sum(ratings.morphs.isna()), sum(ratings_test.morphs.isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find best seed......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c58f3664a0b4b9da4c8a190b2a77c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0\n",
      "280028\n",
      "73093.95535086525 72788.64036922259\n",
      "seed: 4\n",
      "279283\n",
      "72659.20509965229 74530.59328873192\n",
      "seed: 8\n",
      "277420\n",
      "72949.85140907383 73359.04609988058\n",
      "seed: 108\n",
      "276936\n",
      "73050.30235850641 72953.05609298393\n",
      "seed: 128\n",
      "276021\n",
      "73141.69375567777 72588.56850242332\n",
      "seed: 267\n",
      "275940\n",
      "73197.65236750114 72370.5584639531\n",
      "seed: 1069\n",
      "275647\n",
      "73067.3642571775 72884.91745579352\n",
      "seed: 1395\n",
      "275052\n",
      "73194.207448039 72384.66283194325\n",
      "seed: 4843\n",
      "274889\n",
      "73004.69947935347 73136.23770602426\n",
      "seed: 5107\n",
      "274502\n",
      "73130.34417974179 72631.76975883021\n"
     ]
    }
   ],
   "source": [
    "# ### Find best random seed trought rasidual and varience\n",
    "\n",
    "# print('Find best seed......')\n",
    "\n",
    "# min_seed = 0\n",
    "# min_residual = 100000000\n",
    "# for i in tqdm(range(10000)):\n",
    "#     x_data = ratings.morphs.apply(split_word).tolist()\n",
    "#     y_data = ratings.label.tolist()\n",
    "\n",
    "#     x_train_word, x_val_word, y_train, y_val = train_test_split(x_data, y_data,\n",
    "#                                                                 test_size=0.2,\n",
    "#                                                                 random_state=i,\n",
    "#                                                                 stratify=y_data)\n",
    "#     # print(len(y_train), sum(y_train), len(y_val), sum(y_val))\n",
    "\n",
    "#     word_table = list(set([word for words in x_data for word in words]))\n",
    "#     word_table = {word:0 for word in word_table}\n",
    "\n",
    "#     train_counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c in x_train_word]))\n",
    "#     train_table = word_table.copy()\n",
    "#     train_table.update(train_counter)\n",
    "\n",
    "#     val_counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c in x_val_word]))\n",
    "#     val_table = word_table.copy()\n",
    "#     val_table.update(val_counter)\n",
    "\n",
    "#     train_cnt = np.array(list(train_table.values()))\n",
    "#     val_cnt = np.array(list(val_table.values())) * 4\n",
    "    \n",
    "#     residual = np.abs(train_cnt-val_cnt).sum()\n",
    "#     if residual < min_residual:\n",
    "#         min_residual = residual\n",
    "#         min_seed = i\n",
    "#         print('seed:', i)\n",
    "#         print(min_residual)\n",
    "#         print(np.var(train_cnt), np.var(val_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(text):\n",
    "#     return text.split('+')\n",
    "    return [word for word in text.split('+')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train, val data......\n"
     ]
    }
   ],
   "source": [
    "print('Make train, val data......')\n",
    "\n",
    "x_data = ratings.morphs.apply(split_word).tolist()\n",
    "y_data = ratings.label.tolist()\n",
    "\n",
    "x_train_word, x_val_word, y_train, y_val = train_test_split(x_data, y_data,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=108,\n",
    "                                                            stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make test data......\n"
     ]
    }
   ],
   "source": [
    "# Make test data\n",
    "print('Make test data......')\n",
    "x_test_word = ratings_test.morphs.apply(split_word).tolist()\n",
    "y_test = ratings_test.label.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building vocabulary and connecting vocabulary with fasttext embedding\n",
    "https://gluon-nlp.mxnet.io/examples/word_embedding/word_embedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset 기반으로 vocab 생성\n",
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c in x_train_word]))\n",
    "vocab = nlp.Vocab(counter,bos_token=None, eos_token=None, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading fasttext embedding \n",
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.ko')\n",
    "\n",
    "# vocab에 embedding 연결\n",
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.7 s, sys: 132 ms, total: 3.83 s\n",
      "Wall time: 3.81 s\n",
      "Compiler : 546 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# final preprocessing\n",
    "\n",
    "x_train = list(map(lambda sen : [vocab.token_to_idx[token] for token in sen], x_train_word))\n",
    "x_train = pad_sequences(sequences = x_train, maxlen = 30, padding = 'pre', value = 1.)\n",
    "\n",
    "x_val = list(map(lambda sen : [vocab.token_to_idx[token] for token in sen], x_val_word))\n",
    "x_val = pad_sequences(sequences = x_val, maxlen = 30, padding = 'pre', value = 1.)\n",
    "\n",
    "x_test = list(map(lambda sen : [vocab.token_to_idx[token] for token in sen], x_test_word))\n",
    "x_test = pad_sequences(sequences = x_test, maxlen = 30, padding = 'pre', value = 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MorphConv class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphConv:\n",
    "    def __init__(self, X, y, n_of_classes, embedding):\n",
    "        with tf.variable_scope('input_layer'):\n",
    "            self.__X = X\n",
    "            self.__y = y\n",
    "            self.is_training = tf.placeholder(dtype = tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('embedding_layer'):\n",
    "            static_embed = tf.get_variable(name = 'static', initializer = embedding,\n",
    "                                           trainable = False)\n",
    "            non_static_embed = tf.get_variable(name = 'non_static', initializer = embedding,\n",
    "                                               trainable = True)\n",
    "            static_batch = tf.nn.embedding_lookup(params = static_embed, ids = self.__X)\n",
    "            non_static_batch = tf.nn.embedding_lookup(params = non_static_embed, ids = self.__X)\n",
    "            \n",
    "        with tf.variable_scope('convoluion_layer'):\n",
    "            with tf.variable_scope('tri_gram'):\n",
    "                \n",
    "                tri_gram = keras.layers.Conv1D(filters = 100, kernel_size = 3,\n",
    "                                               activation = keras.activations.relu,\n",
    "                                               kernel_initializer = 'he_uniform', padding = 'valid')\n",
    "                static_3 = tri_gram(static_batch)\n",
    "                non_static_3 = tri_gram(non_static_batch)\n",
    "            \n",
    "            with tf.variable_scope('tetra_gram'):\n",
    "                tetra_gram = keras.layers.Conv1D(filters = 100, kernel_size = 4,\n",
    "                                                 activation = keras.activations.relu,\n",
    "                                                 kernel_initializer = 'he_uniform', padding = 'valid')\n",
    "                \n",
    "                static_4 = tetra_gram(static_batch)\n",
    "                non_static_4 = tetra_gram(non_static_batch)\n",
    "            \n",
    "            with tf.variable_scope('penta_gram'):\n",
    "                penta_gram = keras.layers.Conv1D(filters = 100, kernel_size = 5,\n",
    "                                                 activation = keras.activations.relu,\n",
    "                                                 kernel_initializer = 'he_uniform', padding = 'valid')\n",
    "                \n",
    "                static_5 = penta_gram(static_batch)\n",
    "                non_static_5 = penta_gram(non_static_batch)\n",
    "\n",
    "            fmap_3 = tf.reduce_max(static_3 + non_static_3, axis = 1)\n",
    "            fmap_4 = tf.reduce_max(static_4 + non_static_4, axis = 1)\n",
    "            fmap_5 = tf.reduce_max(static_5 + non_static_5, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('output_layer'):\n",
    "            flattened = tf.concat([fmap_3, fmap_4, fmap_5], axis = -1)\n",
    "            score = keras.layers.Dense(units = n_of_classes,\n",
    "                                       kernel_regularizer = keras.regularizers.l2(.7))(flattened)\n",
    "            \n",
    "            self.__score = keras.layers.Dropout(rate = .5)(score, training = self.is_training)\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            ce_loss = tf.losses.sparse_softmax_cross_entropy(labels = self.__y, logits = self.__score)\n",
    "            reg_term = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            self.total_loss = ce_loss + reg_term\n",
    "        \n",
    "        with tf.variable_scope('prediction'):\n",
    "            self.prediction = tf.argmax(self.__score, axis = -1)\n",
    "        \n",
    "    # predict instance method for small dataset\n",
    "    def predict(self, sess, x_data, is_training = False):\n",
    "        feed_prediction = {self.__X : x_data, self.is_training : is_training}\n",
    "        return sess.run(self.prediction, feed_dict = feed_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model of MorphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "lr = 0.003\n",
    "epochs = 30\n",
    "batch_size = 10000\n",
    "total_step = int(x_train.shape[0] / batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 1000000)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "tr_iterator = tr_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size = batch_size)\n",
    "val_iterator = val_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonymous iterator\n",
    "handle = tf.placeholder(dtype = tf.string)\n",
    "iterator = tf.data.Iterator.from_string_handle(string_handle = handle,\n",
    "                                               output_types = tr_iterator.output_types,\n",
    "                                               output_shapes = tr_iterator.output_shapes)\n",
    "x_data, y_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_conv = MorphConv(X = x_data, y = y_data, n_of_classes = 2,\n",
    "                       embedding = vocab.embedding.idx_to_vec.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training op\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "training_op = opt.minimize(loss = morph_conv.total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=30)\n",
    "save_dir = 'checkpoints/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.system('rm -rf '+save_dir+'*ckpt*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "# sess_config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "sess = tf.Session(config = sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tr_handle, val_handle = sess.run(fetches = [tr_iterator.string_handle(), val_iterator.string_handle()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af09a0955a6f49b082f754453f4ef7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   1, train_loss : 2.599, val_loss : 0.993\n",
      "epoch :   2, train_loss : 0.813, val_loss : 0.641\n",
      "epoch :   3, train_loss : 0.605, val_loss : 0.505\n",
      "epoch :   4, train_loss : 0.528, val_loss : 0.463\n",
      "epoch :   5, train_loss : 0.486, val_loss : 0.433\n",
      "epoch :   6, train_loss : 0.459, val_loss : 0.410\n",
      "epoch :   7, train_loss : 0.440, val_loss : 0.396\n",
      "epoch :   8, train_loss : 0.420, val_loss : 0.385\n",
      "epoch :   9, train_loss : 0.406, val_loss : 0.378\n",
      "epoch :  10, train_loss : 0.393, val_loss : 0.372\n",
      "epoch :  11, train_loss : 0.384, val_loss : 0.369\n",
      "epoch :  12, train_loss : 0.371, val_loss : 0.367\n",
      "epoch :  13, train_loss : 0.361, val_loss : 0.366\n",
      "epoch :  14, train_loss : 0.352, val_loss : 0.366\n",
      "epoch :  15, train_loss : 0.345, val_loss : 0.367\n",
      "epoch :  16, train_loss : 0.338, val_loss : 0.368\n",
      "epoch :  17, train_loss : 0.329, val_loss : 0.369\n",
      "epoch :  18, train_loss : 0.319, val_loss : 0.371\n",
      "epoch :  19, train_loss : 0.316, val_loss : 0.375\n",
      "epoch :  20, train_loss : 0.309, val_loss : 0.377\n",
      "epoch :  21, train_loss : 0.302, val_loss : 0.381\n",
      "epoch :  22, train_loss : 0.295, val_loss : 0.384\n",
      "epoch :  23, train_loss : 0.290, val_loss : 0.388\n",
      "epoch :  24, train_loss : 0.286, val_loss : 0.391\n",
      "epoch :  25, train_loss : 0.279, val_loss : 0.395\n",
      "epoch :  26, train_loss : 0.275, val_loss : 0.400\n",
      "epoch :  27, train_loss : 0.271, val_loss : 0.404\n",
      "epoch :  28, train_loss : 0.265, val_loss : 0.409\n",
      "epoch :  29, train_loss : 0.262, val_loss : 0.414\n",
      "epoch :  30, train_loss : 0.256, val_loss : 0.419\n",
      "\n",
      "CPU times: user 1min 44s, sys: 42.4 s, total: 2min 26s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    tr_step = 0\n",
    "    val_step = 0\n",
    "\n",
    "    # for mini-batch training\n",
    "    sess.run(tr_iterator.initializer)    \n",
    "    try:\n",
    "        \n",
    "        while True:\n",
    "            _, train_loss = sess.run(fetches = [training_op, morph_conv.total_loss],\n",
    "                                             feed_dict = {handle : tr_handle, morph_conv.is_training : True})\n",
    "            avg_train_loss += train_loss\n",
    "            tr_step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    # for validation\n",
    "    sess.run(val_iterator.initializer)\n",
    "    try:\n",
    "        while True:\n",
    "            val_loss = sess.run(fetches = morph_conv.total_loss,\n",
    "                                feed_dict = {handle : val_handle, morph_conv.is_training : False})\n",
    "            avg_val_loss += val_loss\n",
    "            val_step += 1\n",
    "    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    avg_train_loss /= tr_step\n",
    "    avg_val_loss /= val_step\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    \n",
    "    saver.save(sess=sess, \n",
    "               save_path=save_dir+str(epoch+1).zfill(3)+'_'+str(int(avg_train_loss*1000)).zfill(4)+'_'+str(int(avg_val_loss*1000)).zfill(4)+'.ckpt')\n",
    "    \n",
    "    print('epoch : {:3}, train_loss : {:.3f}, val_loss : {:.3f}'.format(epoch + 1, avg_train_loss, avg_val_loss))\n",
    "    \n",
    "#     threshold = 5\n",
    "#     if epoch >= 5:\n",
    "#         print([prev_val_loss < avg_val_loss for prev_val_loss \n",
    "#                 in val_loss_hist[epoch-threshold:epoch]])\n",
    "#         if all([prev_val_loss < avg_val_loss for prev_val_loss \n",
    "#                 in val_loss_hist[epoch-threshold:epoch]]):\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/014_0352_0366.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/014_0352_0366.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, save_dir+'014_0352_0366.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size = batch_size)\n",
    "test_iterator = test_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_handle = sess.run(test_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = np.array([])\n",
    "\n",
    "sess.run(test_iterator.initializer)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        y_test_tmp = sess.run(morph_conv.prediction,\n",
    "                            feed_dict = {handle : test_handle,\n",
    "                                         morph_conv.is_training : False})\n",
    "        y_test_hat= np.append(y_test_hat, y_test_tmp)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc : 83.32%\n"
     ]
    }
   ],
   "source": [
    "print('test acc : {:.2%}'.format(np.mean(y_test_hat == np.array(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     1,     1, ...,     1,  1029,    21],\n",
       "       [    1,     1,     1, ...,     1,     1,     0],\n",
       "       [    1,     1,     1, ...,  3033,    22,    19],\n",
       "       ...,\n",
       "       [    1,     1,     1, ..., 19180, 15631,  3145],\n",
       "       [    1,     1,     1, ...,    37,     7,   227],\n",
       "       [    1,     1,     1, ...,   138,    47,    12]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[y_test_hat == np.array(y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
